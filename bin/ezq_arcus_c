#!/bin/bash

# ezq -- easily add pw.x runs to the job queue
# John Wilkinson 26/5/21


function print_help_arcus {
	cat < /dev/tty << EOF
EzQ (EasyQueue) -- Queue runs for pw.x on ARCUS nodes with ease!
-----------------------------------------------------------------

Usage: ezq [-t [runtime]] [-n [nodes]] [-p] [-d] INPUT_FILE

-t --time [runtime]: Runtime in the format HH:MM:SS. Default is 10 hours

-p --partition [long|medium|short|devel]: Partition to run on. You must check that this is compatiable with the time argument -- I won't check for you

-n --nodes [nodes]: Number of nodes requested. Default is 2. All 48 CPUs in a node will be requested.

-m --memory [memory]: memory in GB per CPU. Default is 2 GB per CPU
 
--priority: `if [ $priority -eq 1 ]; then echo 'Ignored (change ezq configuration to allow one to use this)'; 
			else echo 'Run in ARC priority mode'; fi`
 
INPUT_FILE: Input file to run. Supports quantum espresso .pwi files, and python .py files.
EOF
}

nodes=2 # number of nodes to use 
memory=2 # memory per core
run_time='10:00:00' # runtime
partition='medium' # partition

# parse the arguments
while [[ $# -gt 0 ]]
do
key="$1"

case $key in
    -h|--help)
            print_help_arcus
            exit
	    ;;
    -t|--time)
    run_time="$2"
    shift # past argument
    shift # past value
    ;;
    -m|--memory)
    memory="$2"
    shift # past argument
    shift # past value
    ;;
    -n|--nodes)
    nodes="$2"
    shift # past argument
    shift # past value
    ;;
    -p|--partition)
    partition="$2" 
    shift # past argument
    shift # past value
    ;;
    --priority)
    priority=1
    shift # past argument
    ;;
    *)    #  file name
    input_file="$1" # pwi file 
    shift # past argument
    ;;
esac
done

if [ -z "$input_file" ]
then
	echo "No input file given. Aborting";
	exit
fi

# now adjust the number of processors to match the number of nodes*proc per node

file_root="${input_file%.*}"
file_extension="${input_file##*.}"
file_number=$(echo "$input_file" | grep -Eo "\.[0-9]{1,}\." | grep -Eo "[0-9]{1,}")
pwo_file="${file_root}.pwo"
run_file="${file_root}.run"

# check the time is sensible
if [ $partition = 'devel' ]
then
	echo $run_time | grep -E "00\:[0-1]*[0-9]\:[0-5][0-9]" > /dev/null
	if [ $? -ne 0 ]
	then
		echo "Setting time to 10 mins for the development cluster"
		run_time='00:10:00'
	fi
else
	echo $run_time | grep -E "[0-9]*[0-9]\:[0-5]*[0-9]\:[0-5][0-9]" > /dev/null
	if [ $? -ne 0 ]
	then
		echo "Time format invalid. The time format must be HH:MM:SS"
		exit
	fi
fi

cat > /dev/tty << EOF
Setting up an ARCUS job with $nodes nodes ($memory GB per CPU), for a duration of $run_time, on the partition $partition.
The input file is $input_file `if [ $file_extension = 'pwi' ]; then echo ", and the output will be saved to $pwo_file."; fi`
EOF

cores=$(( 48*$nodes))

cat > $run_file << EOF
#!/bin/bash

#======================================================================
`if [ $priority -eq 1 ]; then echo '#SBATCH --qos=priority'; fi`
#SBATCH --partition=${partition}
#SBATCH --ntasks-per-node=48
#SBATCH --mem-per-cpu=${memory}G
#SBATCH --job-name=${file_number}${input_file}
#SBATCH --time=$run_time
#SBATCH --nodes=$nodes
#======================================================================

module purge

cd $PWD

cores=$cores

EOF
 
if [ $file_extension = 'py' ] 
then
	for module in ${arcus_c_py_modules[@]}; do
		echo "module load $module" >> $run_file
	done
cat >> $run_file << EOF
source activate $python_env

python3 $input_file -n \$cores
EOF
elif [ $file_extension = 'pwi' ]
then
	for module in ${arcus_c_qe_modules[@]}; do
		echo "module load $module" >> $run_file
	done
cat >> $run_file << EOF
mpirun \$MPI_HOSTS pw.x -in $input_file > $pwo_file
EOF
fi

sbatch $run_file
